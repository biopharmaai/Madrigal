{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from madrigal.utils import DATA_DIR, BASE_DIR\n",
    "from madrigal.evaluate.predict import get_data_for_analysis_all_drugs\n",
    "from madrigal.models.models import NovelDDIEncoder, NovelDDIMultilabel\n",
    "from madrigal.utils import to_device\n",
    "from madrigal.evaluate.eval_utils import get_evaluate_masks\n",
    "\n",
    "OUTPUT_DIR = DATA_DIR + \"polypharmacy_new/DrugBank/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11607, 86)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drug_metadata_nash = pd.read_pickle(BASE_DIR + \"processed_data/views_features_new/combined_metadata_nash.pkl\")\n",
    "drug_metadata_nash.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def classwise_normalized_rank_3d_numpy(tensor):\n",
    "    # flatten the tensor while maintaining the class dimension\n",
    "    # print(\"Flattening...\")\n",
    "    flat_tensor = tensor.reshape(tensor.shape[0], -1)\n",
    "    \n",
    "    # compute the ranks\n",
    "    # print(\"Computing ranks (via argsort), will take quite a while...\")\n",
    "    # start = time()\n",
    "    \n",
    "    if tensor.shape[0] > 1:\n",
    "        flat_rank = flat_tensor.argsort(axis=1).argsort(axis=1) + 1\n",
    "    else:\n",
    "        temp = flat_tensor.argsort(axis=1)\n",
    "        flat_rank = np.empty_like(temp)\n",
    "        flat_rank[0, temp] = np.arange(flat_rank.shape[1]) + 1\n",
    "        del temp\n",
    "\n",
    "    # end = time()\n",
    "    # print(f\"Finished computing ranks in {(end - start):.4f} seconds.\")\n",
    "    \n",
    "    # normalize the ranks\n",
    "    normalized_rank = flat_rank / (tensor.shape[1] * (tensor.shape[2] - 1) / 2)\n",
    "\n",
    "    # reshape back to the original shape\n",
    "    return normalized_rank.reshape(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE = True  # NOTE: Only generating embeddings and scores for 200 drugs. Normalization is also within pairs of these drugs, which can differ significantly from the full dataset.\n",
    "data_source = 'DrugBank'\n",
    "split_method = 'split_by_pairs'\n",
    "repeat = None\n",
    "eval_type = 'full_full'\n",
    "finetune_mode = 'str_str+random_sample'\n",
    "split_output_dir = BASE_DIR + f\"model_output/{data_source}/{split_method}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = 'drawn-grass-4'  # DrugBank (trained on all data, seed=1)\n",
    "# checkpoint = 'misty-oath-5'  # DrugBank (trained on all data, seed=0)\n",
    "# checkpoint = 'whole-fog-7'  # DrugBank (trained on all data, seed=99)\n",
    "# checkpoint = 'snowy-serenity-8'  # DrugBank (trained on all data, seed=42)\n",
    "checkpoint = 'revived-aardvark-8'  # DrugBank (trained on all data, seed=2)\n",
    "\n",
    "checkpoint_dir = BASE_DIR + f'model_output/{data_source}/{split_method}/{checkpoint}/'\n",
    "\n",
    "# epoch = None\n",
    "epoch = 700\n",
    "kg_encoder = 'hgt'\n",
    "if epoch is None:\n",
    "    ckpt_path = checkpoint_dir + \"best_model.pt\"\n",
    "else:\n",
    "    ckpt_path = checkpoint_dir + f\"checkpoint_{epoch}.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate embeddings and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, batch, label_map = get_data_for_analysis_all_drugs(\n",
    "    data_source=data_source, \n",
    "    kg_encoder=kg_encoder, \n",
    "    split_method=split_method, \n",
    "    repeat=repeat, \n",
    "    path_base=DATA_DIR, \n",
    "    checkpoint=ckpt_path, \n",
    "    first_num_drugs=200 if EXAMPLE else drug_metadata_nash.shape[0], \n",
    "    add_specific_drugs=\"nash\"\n",
    ")\n",
    "\n",
    "ddi_labels = batch['edge_indices']['label']\n",
    "ddi_pos_neg_samples = batch['edge_indices']['pos_neg'].float()\n",
    "true_ddis = ddi_pos_neg_samples\n",
    "label_map_valid = np.array(label_map)[np.unique(ddi_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pretrained structure encoder\n",
      "Using pretrained KG encoder\n",
      "Using pretrained CV encoder\n",
      "Using pretrained TX encoder\n",
      "INCOMP_KEYS (make sure these contain what you expected):\n",
      "%s {   'Missing keys': [   'covariates_embeddings.0.weight'],\n",
      "    'Unexpected_keys': [   ]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/data1/hms/dbmi/zitnik/lab/users/yeh803/Archived/miniforge3/envs/madrigal_env/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "if epoch is None:\n",
    "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "else:\n",
    "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    \n",
    "encoder = NovelDDIEncoder(**checkpoint['encoder_configs'])\n",
    "model = NovelDDIMultilabel(encoder, **checkpoint['model_configs'])\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "batch_head = to_device(batch['head'], device)  # dict\n",
    "batch_tail = to_device(batch['tail'], device)\n",
    "batch_kg = to_device(batch['kg'], device)\n",
    "head_masks_base = batch['head']['masks']\n",
    "tail_masks_base = batch['tail']['masks']\n",
    "\n",
    "masks_head, masks_tail = get_evaluate_masks(head_masks_base, tail_masks_base, eval_type, finetune_mode, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_drugs = batch_head['drugs']\n",
    "head_mol_strs = batch_head['strs']\n",
    "head_cv = batch_head['cv']\n",
    "head_tx_all_cell_lines = batch_head['tx']\n",
    "head_masks = masks_head\n",
    "\n",
    "tail_drugs = batch_tail['drugs']\n",
    "tail_mol_strs = batch_tail['strs']\n",
    "tail_cv = batch_tail['cv']\n",
    "tail_tx_all_cell_lines = batch_tail['tx']\n",
    "tail_masks = masks_tail\n",
    "\n",
    "with torch.no_grad():\n",
    "    z_full = model.encoder(head_drugs, head_masks, head_mol_strs, batch_kg, head_cv, head_tx_all_cell_lines)\n",
    "    if model.normalize:\n",
    "        z_full = F.normalize(z_full)\n",
    "\n",
    "torch.save(z_full.detach().cpu(), f\"{checkpoint_dir}/{data_source}_drug_embeddings_full.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "120\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "fp = np.memmap(\n",
    "    f\"{checkpoint_dir}/{data_source}_drugs_raw_scores_{epoch}.raw\", \n",
    "    dtype=np.float32, mode=\"w+\", shape=(label_map.shape[0], z_full.shape[0], z_full.shape[0])\n",
    ")\n",
    "\n",
    "start_idx = 0\n",
    "for start, end in zip(np.arange(0, len(label_map), 30), np.arange(0, len(label_map), 30)[1:].tolist() + [len(label_map)]):\n",
    "    print(start)\n",
    "    label_range = (start, end)\n",
    "    with torch.no_grad():\n",
    "        pred_scores = model.decoder(z_full, z_full, label_range).detach().cpu().numpy()\n",
    "    fp[start:end, :, :] = pred_scores\n",
    "\n",
    "with open(f\"{checkpoint_dir}/{data_source}_drugs_raw_scores_{epoch}.npy\", \"wb\") as f:\n",
    "    np.save(f, fp)\n",
    "    \n",
    "fp.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert scores to normalized ranks\n",
    "\n",
    "Normalize scores to ranks, normalized to [0, 1]. This step can take a few hours to run for a large number of drugs.\n",
    "\n",
    "Script version can be found at `notebooks/normalize_scores.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_scores = np.load(f\"{checkpoint_dir}/{data_source}_drugs_raw_scores_{epoch}.npy\", mmap_mode=\"r\")\n",
    "raw_scores_norm = np.memmap(f\"{checkpoint_dir}/{data_source}_drugs_normalized_ranks_{epoch}.raw\", mode=\"w+\", dtype=np.float32, shape=raw_scores.shape)\n",
    "\n",
    "mask_indices = np.vstack(np.triu_indices(raw_scores.shape[1], k=0, m=raw_scores.shape[2]))\n",
    "interval = 1\n",
    "def run_slice(tup):\n",
    "    st = time()\n",
    "    start, end = tup\n",
    "    raw_scores_slice = raw_scores[start:end, :, :]\n",
    "    raw_scores_slice = raw_scores_slice.copy()\n",
    "    raw_scores_slice[:, mask_indices[0], mask_indices[1]] = 1e7\n",
    "    raw_scores_slice_norm = classwise_normalized_rank_3d_numpy(raw_scores_slice)\n",
    "    raw_scores_slice_norm[:, mask_indices[0], mask_indices[1]] = 0\n",
    "    raw_scores_slice_norm = raw_scores_slice_norm + raw_scores_slice_norm.swapaxes(1, 2)\n",
    "    # assert raw_scores_slice_norm.max() < 1e7\n",
    "    raw_scores_norm[start:end, :, :] = raw_scores_slice_norm\n",
    "    e = time()\n",
    "    # print(f\"Finished normalizing class {start} in {((e - st) / 60):.4f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(raw_scores).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "st = time()\n",
    "with Pool() as pool:\n",
    "    pool.map(run_slice, zip(np.arange(0, raw_scores.shape[0], 1), np.arange(0, raw_scores.shape[0], 1)[1:].tolist() + [raw_scores.shape[0]]))\n",
    "e = time()\n",
    "print(f\"Takes {(e-st):.4f} seconds to run the score normalization.\")\n",
    "\n",
    "with open(f\"{checkpoint_dir}/{data_source}_drugs_normalized_ranks_{epoch}.npy\", \"wb\") as f:\n",
    "    np.save(f, raw_scores_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_scores_norm.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Across runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric mean-aggregate normalized ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "Takes 3015.0882 seconds to run gmean for 5 normalized ranks.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "checkpoints = [\n",
    "    \"drawn-grass-4\",  # DrugBank (trained on all data, seed=1)\n",
    "    \"misty-oath-5\",  # DrugBank (trained on all data, seed=0)\n",
    "    \"whole-fog-7\",  # DrugBank (trained on all data, seed=99)\n",
    "    \"snowy-serenity-8\",  # DrugBank (trained on all data, seed=42)\n",
    "    \"revived-aardvark-8\",  # DrugBank (trained on all data, seed=2)\n",
    "]\n",
    "\n",
    "normalized_ranks_list = []\n",
    "for checkpoint in checkpoints:\n",
    "    eval_type = 'full_full'\n",
    "    checkpoint_dir = BASE_DIR + f'model_output/{data_source}/{split_method}/{checkpoint}/'\n",
    "    normalized_ranks = np.load(f\"{checkpoint_dir}/{data_source}_drugs_normalized_ranks_{epoch}.npy\", mmap_mode=\"r\")\n",
    "    normalized_ranks_list.append(normalized_ranks)\n",
    "\n",
    "gmean_fp = np.memmap(\n",
    "    split_output_dir + f\"{data_source}_drugs_normalized_ranks_{epoch}_gmean.raw\", \n",
    "    dtype=np.float32, mode=\"w+\", shape=(normalized_ranks.shape[0], normalized_ranks.shape[1], normalized_ranks.shape[2])\n",
    ")\n",
    "\n",
    "start_idx = 0\n",
    "interval = 10\n",
    "\n",
    "st = time()\n",
    "for start, end in zip(\n",
    "    np.arange(0, normalized_ranks.shape[0], interval), \n",
    "    np.arange(0, normalized_ranks.shape[0], interval)[1:].tolist() + [normalized_ranks.shape[0]]\n",
    "):\n",
    "    print(start)\n",
    "    gmean_fp[start:end, :, :] = gmean(np.stack([ranks[start:end, :, :] for ranks in normalized_ranks_list], axis=-1), axis=-1)\n",
    "e = time()\n",
    "print(f\"Takes {(e-st):.4f} seconds to run gmean.\")\n",
    "\n",
    "with open(split_output_dir + f\"{data_source}_drugs_normalized_ranks_{epoch}_gmean.npy\", \"wb\") as f:\n",
    "    np.save(f, gmean_fp)\n",
    "    \n",
    "gmean_fp.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-normalize (normalized) ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished normalizing class 28 in 1.7004 minutes.\n",
      "Finished normalizing class 60 in 1.7130 minutes.\n",
      "Finished normalizing class 58 in 1.7616 minutes.\n",
      "Finished normalizing class 62 in 1.7917 minutes.\n",
      "Finished normalizing class 56 in 2.2525 minutes.\n",
      "Finished normalizing class 38 in 2.7902 minutes.\n",
      "Finished normalizing class 14 in 2.7951 minutes.\n",
      "Finished normalizing class 32 in 2.8403 minutes.\n",
      "Finished normalizing class 44 in 2.8908 minutes.\n",
      "Finished normalizing class 18 in 2.9051 minutes.\n",
      "Finished normalizing class 0 in 2.9185 minutes.\n",
      "Finished normalizing class 2 in 2.9214 minutes.\n",
      "Finished normalizing class 20 in 2.9364 minutes.\n",
      "Finished normalizing class 16 in 3.0938 minutes.\n",
      "Finished normalizing class 36 in 3.2349 minutes.\n",
      "Finished normalizing class 22 in 3.3714 minutes.\n",
      "Finished normalizing class 40 in 3.3913 minutes.\n",
      "Finished normalizing class 24 in 3.4244 minutes.\n",
      "Finished normalizing class 4 in 3.4342 minutes.\n",
      "Finished normalizing class 8 in 3.4389 minutes.\n",
      "Finished normalizing class 12 in 3.4566 minutes.\n",
      "Finished normalizing class 34 in 3.4621 minutes.\n",
      "Finished normalizing class 52 in 3.4670 minutes.\n",
      "Finished normalizing class 48 in 3.5559 minutes.\n",
      "Finished normalizing class 50 in 3.5639 minutes.\n",
      "Finished normalizing class 6 in 3.5664 minutes.\n",
      "Finished normalizing class 10 in 3.5783 minutes.\n",
      "Finished normalizing class 26 in 3.5784 minutes.\n",
      "Finished normalizing class 46 in 3.5875 minutes.\n",
      "Finished normalizing class 54 in 3.5876 minutes.\n",
      "Finished normalizing class 29 in 1.9161 minutes.\n",
      "Finished normalizing class 42 in 3.6254 minutes.\n",
      "Finished normalizing class 30 in 3.6347 minutes.\n",
      "Finished normalizing class 61 in 1.9371 minutes.\n",
      "Finished normalizing class 59 in 1.9097 minutes.\n",
      "Finished normalizing class 63 in 1.9303 minutes.\n",
      "Finished normalizing class 57 in 1.4766 minutes.\n",
      "Finished normalizing class 15 in 2.1103 minutes.\n",
      "Finished normalizing class 39 in 2.4993 minutes.\n",
      "Finished normalizing class 33 in 2.6230 minutes.\n",
      "Finished normalizing class 3 in 2.5480 minutes.\n",
      "Finished normalizing class 1 in 2.5662 minutes.\n",
      "Finished normalizing class 19 in 2.5893 minutes.\n",
      "Finished normalizing class 21 in 2.5646 minutes.\n",
      "Finished normalizing class 45 in 2.9036 minutes.\n",
      "Finished normalizing class 9 in 2.4797 minutes.\n",
      "Finished normalizing class 5 in 2.4845 minutes.\n",
      "Finished normalizing class 17 in 2.8527 minutes.\n",
      "Finished normalizing class 68 in 2.3934 minutes.\n",
      "Finished normalizing class 64 in 2.5152 minutes.\n",
      "Finished normalizing class 27 in 2.5666 minutes.\n",
      "Finished normalizing class 11 in 2.5752 minutes.\n",
      "Finished normalizing class 7 in 2.5899 minutes.\n",
      "Finished normalizing class 49 in 2.6070 minutes.\n",
      "Finished normalizing class 53 in 2.7131 minutes.\n",
      "Finished normalizing class 13 in 2.7259 minutes.\n",
      "Finished normalizing class 47 in 2.5957 minutes.\n",
      "Finished normalizing class 31 in 2.5591 minutes.\n",
      "Finished normalizing class 66 in 2.5364 minutes.\n",
      "Finished normalizing class 51 in 2.6331 minutes.\n",
      "Finished normalizing class 70 in 2.4682 minutes.\n",
      "Finished normalizing class 55 in 2.6165 minutes.\n",
      "Finished normalizing class 72 in 2.4745 minutes.\n",
      "Finished normalizing class 25 in 2.8022 minutes.\n",
      "Finished normalizing class 23 in 2.9151 minutes.\n",
      "Finished normalizing class 43 in 2.6606 minutes.\n",
      "Finished normalizing class 37 in 3.0526 minutes.\n",
      "Finished normalizing class 41 in 2.8970 minutes.\n",
      "Finished normalizing class 35 in 2.8416 minutes.\n",
      "Finished normalizing class 74 in 1.4579 minutes.\n",
      "Finished normalizing class 76 in 1.0760 minutes.\n",
      "Finished normalizing class 78 in 1.2308 minutes.\n",
      "Finished normalizing class 80 in 1.2467 minutes.\n",
      "Finished normalizing class 82 in 1.2525 minutes.\n",
      "Finished normalizing class 84 in 1.2570 minutes.\n",
      "Finished normalizing class 86 in 1.2652 minutes.\n",
      "Finished normalizing class 88 in 1.4917 minutes.\n",
      "Finished normalizing class 90 in 1.3947 minutes.\n",
      "Finished normalizing class 94 in 2.6477 minutes.\n",
      "Finished normalizing class 92 in 2.6838 minutes.\n",
      "Finished normalizing class 96 in 2.8637 minutes.\n",
      "Finished normalizing class 69 in 2.9395 minutes.\n",
      "Finished normalizing class 100 in 2.9383 minutes.\n",
      "Finished normalizing class 65 in 3.1695 minutes.\n",
      "Finished normalizing class 108 in 3.1538 minutes.\n",
      "Finished normalizing class 120 in 3.3541 minutes.\n",
      "Finished normalizing class 75 in 3.2859 minutes.\n",
      "Finished normalizing class 104 in 3.4815 minutes.\n",
      "Finished normalizing class 112 in 3.4724 minutes.\n",
      "Finished normalizing class 116 in 3.4749 minutes.\n",
      "Finished normalizing class 124 in 3.4605 minutes.\n",
      "Finished normalizing class 67 in 3.6020 minutes.\n",
      "Finished normalizing class 83 in 3.0536 minutes.\n",
      "Finished normalizing class 91 in 2.5151 minutes.\n",
      "Finished normalizing class 89 in 2.5492 minutes.\n",
      "Finished normalizing class 71 in 3.6556 minutes.\n",
      "Finished normalizing class 122 in 3.5658 minutes.\n",
      "Finished normalizing class 81 in 3.1260 minutes.\n",
      "Finished normalizing class 87 in 3.0833 minutes.\n",
      "Finished normalizing class 77 in 3.4913 minutes.\n",
      "Finished normalizing class 85 in 3.1126 minutes.\n",
      "Finished normalizing class 73 in 3.6738 minutes.\n",
      "Finished normalizing class 114 in 3.6791 minutes.\n",
      "Finished normalizing class 118 in 3.6021 minutes.\n",
      "Finished normalizing class 79 in 3.1841 minutes.\n",
      "Finished normalizing class 98 in 3.7861 minutes.\n",
      "Finished normalizing class 102 in 3.7780 minutes.\n",
      "Finished normalizing class 126 in 3.6380 minutes.\n",
      "Finished normalizing class 106 in 3.7664 minutes.\n",
      "Finished normalizing class 110 in 3.7573 minutes.\n",
      "Finished normalizing class 93 in 1.4350 minutes.\n",
      "Finished normalizing class 95 in 1.4501 minutes.\n",
      "Finished normalizing class 128 in 1.2772 minutes.\n",
      "Finished normalizing class 97 in 1.2999 minutes.\n",
      "Finished normalizing class 101 in 1.4690 minutes.\n",
      "Finished normalizing class 109 in 1.5318 minutes.\n",
      "Finished normalizing class 130 in 1.7219 minutes.\n",
      "Finished normalizing class 144 in 2.0158 minutes.\n",
      "Finished normalizing class 121 in 2.5234 minutes.\n",
      "Finished normalizing class 132 in 2.5526 minutes.\n",
      "Finished normalizing class 117 in 2.5200 minutes.\n",
      "Finished normalizing class 115 in 2.3401 minutes.\n",
      "Finished normalizing class 113 in 2.5669 minutes.\n",
      "Finished normalizing class 125 in 2.4945 minutes.\n",
      "Finished normalizing class 105 in 2.6062 minutes.\n",
      "Finished normalizing class 111 in 2.3194 minutes.\n",
      "Finished normalizing class 134 in 2.5663 minutes.\n",
      "Finished normalizing class 127 in 2.4251 minutes.\n",
      "Finished normalizing class 140 in 2.5412 minutes.\n",
      "Finished normalizing class 146 in 2.5672 minutes.\n",
      "Finished normalizing class 136 in 2.6336 minutes.\n",
      "Finished normalizing class 154 in 2.5481 minutes.\n",
      "Finished normalizing class 99 in 2.7638 minutes.\n",
      "Finished normalizing class 152 in 2.8514 minutes.\n",
      "Finished normalizing class 156 in 2.6973 minutes.\n",
      "Finished normalizing class 148 in 2.8784 minutes.\n",
      "Finished normalizing class 138 in 2.9121 minutes.\n",
      "Finished normalizing class 107 in 2.8011 minutes.\n",
      "Finished normalizing class 142 in 2.9020 minutes.\n",
      "Finished normalizing class 129 in 2.4709 minutes.\n",
      "Finished normalizing class 119 in 2.9460 minutes.\n",
      "Finished normalizing class 123 in 2.9845 minutes.Finished normalizing class 150 in 2.9638 minutes.\n",
      "\n",
      "Finished normalizing class 103 in 2.8999 minutes.\n",
      "Finished normalizing class 131 in 1.8158 minutes.\n",
      "Finished normalizing class 145 in 0.9926 minutes.\n",
      "Finished normalizing class 133 in 1.1637 minutes.\n",
      "Finished normalizing class 141 in 1.0597 minutes.\n",
      "Finished normalizing class 135 in 1.1148 minutes.\n",
      "Finished normalizing class 137 in 1.0765 minutes.\n",
      "Finished normalizing class 147 in 1.0976 minutes.\n",
      "Finished normalizing class 155 in 1.0900 minutes.\n",
      "Finished normalizing class 157 in 0.9976 minutes.\n",
      "Finished normalizing class 151 in 0.9106 minutes.\n",
      "Finished normalizing class 139 in 1.0154 minutes.\n",
      "Finished normalizing class 143 in 1.0206 minutes.\n",
      "Finished normalizing class 153 in 1.0428 minutes.\n",
      "Finished normalizing class 149 in 1.0315 minutes.\n",
      "Takes 837.6621 seconds to run the score normalization.\n"
     ]
    }
   ],
   "source": [
    "gmean_ranks = np.load(split_output_dir + f\"{data_source}_drugs_normalized_ranks_{epoch}_gmean.npy\", mmap_mode=\"r\")\n",
    "\n",
    "gmean_ranks_norm = np.memmap(\n",
    "    split_output_dir + f\"{data_source}_drugs_normalized_ranks.raw\", \n",
    "    mode=\"w+\", dtype=np.float32, shape=gmean_ranks.shape\n",
    ")\n",
    "mask_indices = np.vstack(np.triu_indices(gmean_ranks.shape[1], k=0, m=gmean_ranks.shape[2]))\n",
    "\n",
    "def run_slice(tup):\n",
    "    st = time()\n",
    "    start, end = tup\n",
    "    gmean_ranks_slice = gmean_ranks[start:end, :, :]\n",
    "    gmean_ranks_slice = gmean_ranks_slice.copy()\n",
    "    gmean_ranks_slice[:, mask_indices[0], mask_indices[1]] = 1e7\n",
    "    gmean_ranks_slice_norm = classwise_normalized_rank_3d_numpy(gmean_ranks_slice)\n",
    "    gmean_ranks_slice_norm[:, mask_indices[0], mask_indices[1]] = 0\n",
    "    gmean_ranks_slice_norm = gmean_ranks_slice_norm + gmean_ranks_slice_norm.swapaxes(1, 2)\n",
    "    # assert gmean_ranks_slice_norm.max() < 1e7\n",
    "    gmean_ranks_norm[start:end, :, :] = gmean_ranks_slice_norm\n",
    "    e = time()\n",
    "    # print(f\"Finished normalizing class {start} in {((e - st) / 60):.4f} minutes.\")\n",
    "\n",
    "st = time()\n",
    "with Pool() as pool:\n",
    "    pool.map(run_slice, zip(np.arange(0, gmean_ranks.shape[0], 1), np.arange(0, gmean_ranks.shape[0], 1)[1:].tolist() + [gmean_ranks.shape[0]]))\n",
    "e = time()\n",
    "print(f\"Takes {(e-st):.4f} seconds to run score normalization.\")\n",
    "\n",
    "with open(split_output_dir + f\"{data_source}_drugs_normalized_ranks.npy\", \"wb\") as f:\n",
    "    np.save(f, gmean_ranks_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([[[0.        , 0.6872362 , 0.12949748, ..., 0.30432162,\n",
       "          0.37869346, 0.45221105],\n",
       "         [0.6872362 , 0.        , 0.27125627, ..., 0.37366834,\n",
       "          0.46492463, 0.49110553],\n",
       "         [0.12949748, 0.27125627, 0.        , ..., 0.30708542,\n",
       "          0.598995  , 0.330201  ],\n",
       "         ...,\n",
       "         [0.30432162, 0.37366834, 0.30708542, ..., 0.        ,\n",
       "          0.44246233, 0.41954774],\n",
       "         [0.37869346, 0.46492463, 0.598995  , ..., 0.44246233,\n",
       "          0.        , 0.680603  ],\n",
       "         [0.45221105, 0.49110553, 0.330201  , ..., 0.41954774,\n",
       "          0.680603  , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.33954775, 0.68708545, ..., 0.67577887,\n",
       "          0.7384925 , 0.8203518 ],\n",
       "         [0.33954775, 0.        , 0.528995  , ..., 0.2919598 ,\n",
       "          0.41592965, 0.49537688],\n",
       "         [0.68708545, 0.528995  , 0.        , ..., 0.8427136 ,\n",
       "          0.8675879 , 0.8438191 ],\n",
       "         ...,\n",
       "         [0.67577887, 0.2919598 , 0.8427136 , ..., 0.        ,\n",
       "          0.82798994, 0.91276383],\n",
       "         [0.7384925 , 0.41592965, 0.8675879 , ..., 0.82798994,\n",
       "          0.        , 0.9211055 ],\n",
       "         [0.8203518 , 0.49537688, 0.8438191 , ..., 0.91276383,\n",
       "          0.9211055 , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.28723618, 0.528995  , ..., 0.25884423,\n",
       "          0.44055277, 0.54346734],\n",
       "         [0.28723618, 0.        , 0.60130656, ..., 0.46954775,\n",
       "          0.68095475, 0.6051256 ],\n",
       "         [0.528995  , 0.60130656, 0.        , ..., 0.3732161 ,\n",
       "          0.46266332, 0.931608  ],\n",
       "         ...,\n",
       "         [0.25884423, 0.46954775, 0.3732161 , ..., 0.        ,\n",
       "          0.4683417 , 0.751407  ],\n",
       "         [0.44055277, 0.68095475, 0.46266332, ..., 0.4683417 ,\n",
       "          0.        , 0.7488442 ],\n",
       "         [0.54346734, 0.6051256 , 0.931608  , ..., 0.751407  ,\n",
       "          0.7488442 , 0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.        , 0.739598  , 0.6020603 , ..., 0.78231156,\n",
       "          0.7638191 , 0.3743216 ],\n",
       "         [0.739598  , 0.        , 0.56120604, ..., 0.710201  ,\n",
       "          0.73693466, 0.31045225],\n",
       "         [0.6020603 , 0.56120604, 0.        , ..., 0.24427135,\n",
       "          0.27356783, 0.40015075],\n",
       "         ...,\n",
       "         [0.78231156, 0.710201  , 0.24427135, ..., 0.        ,\n",
       "          0.44914573, 0.32150754],\n",
       "         [0.7638191 , 0.73693466, 0.27356783, ..., 0.44914573,\n",
       "          0.        , 0.47919598],\n",
       "         [0.3743216 , 0.31045225, 0.40015075, ..., 0.32150754,\n",
       "          0.47919598, 0.        ]],\n",
       "\n",
       "        [[0.        , 0.24492462, 0.43683416, ..., 0.5527638 ,\n",
       "          0.61241204, 0.8149749 ],\n",
       "         [0.24492462, 0.        , 0.3182412 , ..., 0.48090452,\n",
       "          0.508995  , 0.718794  ],\n",
       "         [0.43683416, 0.3182412 , 0.        , ..., 0.7564824 ,\n",
       "          0.7425126 , 0.79924625],\n",
       "         ...,\n",
       "         [0.5527638 , 0.48090452, 0.7564824 , ..., 0.        ,\n",
       "          0.90331656, 0.94758797],\n",
       "         [0.61241204, 0.508995  , 0.7425126 , ..., 0.90331656,\n",
       "          0.        , 0.8754774 ],\n",
       "         [0.8149749 , 0.718794  , 0.79924625, ..., 0.94758797,\n",
       "          0.8754774 , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.35517588, 0.9334673 , ..., 0.6879397 ,\n",
       "          0.6149246 , 0.15286432],\n",
       "         [0.35517588, 0.        , 0.9236181 , ..., 0.5845226 ,\n",
       "          0.5890955 , 0.11201005],\n",
       "         [0.9334673 , 0.9236181 , 0.        , ..., 0.91562814,\n",
       "          0.09628141, 0.4640201 ],\n",
       "         ...,\n",
       "         [0.6879397 , 0.5845226 , 0.91562814, ..., 0.        ,\n",
       "          0.41100502, 0.16959798],\n",
       "         [0.6149246 , 0.5890955 , 0.09628141, ..., 0.41100502,\n",
       "          0.        , 0.25703517],\n",
       "         [0.15286432, 0.11201005, 0.4640201 , ..., 0.16959798,\n",
       "          0.25703517, 0.        ]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gmean_ranks_norm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "madrigal_env (cuda11.7)",
   "language": "python",
   "name": "madrigal_env_cuda117"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
